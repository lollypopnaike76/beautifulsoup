{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c09e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\probook 430 g3\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3fa91",
   "metadata": {},
   "source": [
    "# Import requires Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41c9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374dfb2",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 Indian movies’ data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff70fe",
   "metadata": {},
   "source": [
    "# send get request to the webpage server to get the source code of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc48186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.imdb.com/list/ls056092300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20082b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dispaly response\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2204492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the content of Html using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "# dispaly content\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f81ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the movies\n",
    "movies = soup.find_all('div',class_='lister-item-content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc50c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and empty list\n",
    "data = []\n",
    "for movie in movies:\n",
    "    text = movie.text.strip()\n",
    "    name = movie.find('a').text.strip()\n",
    "    rating = movie.find('span',class_='ipl-rating-star__rating').text.strip()\n",
    "    year = movie.find('span',class_='lister-item-year').text.strip()\n",
    "    data.append({'Name':name,'Rating':rating,'Year':year})\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8d3b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating    Year\n",
      "0                     Ship of Theseus      8  (2012)\n",
      "1                              Iruvar    8.4  (1997)\n",
      "2                     Kaagaz Ke Phool    7.8  (1959)\n",
      "3   Lagaan: Once Upon a Time in India    8.1  (2001)\n",
      "4                     Pather Panchali    8.2  (1955)\n",
      "..                                ...    ...     ...\n",
      "95                   The World of Apu    8.4  (1959)\n",
      "96                        Kanchivaram    8.2  (2008)\n",
      "97                    Monsoon Wedding    7.3  (2001)\n",
      "98                              Black    8.1  (2005)\n",
      "99                            Deewaar      8  (1975)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame from extraced Movie\n",
    "df = pd.DataFrame(data, columns=[\"Name\",\"Rating\",\"Year\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282243b1",
   "metadata": {},
   "source": [
    "# 2) Write a python program to scrape product name, price and discounts from "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcc76f",
   "metadata": {},
   "source": [
    "# send get request to the webpage server to get the source code of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84902f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d60d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://peachmode.com/search?q=bags ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c042d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dispaly response\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b81519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the content of Html using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "# dispaly content\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebd0ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the product data\n",
    "\n",
    "products = soup.find_all('div',class_='st-col-xs-12 st-col-md-7')\n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9baecdcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\3000287486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mproduct\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproducts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'product-name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdiscount_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'discounted-price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# creating am empty list\n",
    "data = []\n",
    "for product in products:\n",
    "    text = product.text.strip()\n",
    "    name = product.find('a',class_='product-name').text.strip()\n",
    "    price = product.find('span',class_='price').text.strip()\n",
    "    discount_tag = product.find('div',class_='discounted-price')\n",
    "    discount = discount_tag.find('span').text.strip() \n",
    "    data.append({'Name':name,'Price':price,'Discount':discount})\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a5222ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Price, Discount_tag]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame from extracted product\n",
    "df = pd.DataFrame(data, columns=[\"Name\",\"Price\",\"Discount_tag\"])\n",
    "                  \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f4ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4c70fc1",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca9f6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe5408",
   "metadata": {},
   "source": [
    "# You have to scrape: \n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "712b227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending a GET request to the  url \n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    " \n",
    "# see content in page5\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6547909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display response\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3001f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the content of Html using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content,'html.parser') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da0e6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8fca07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap team names\n",
    "team = soup.find_all(\"span\",class_='u-hide-phablet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "604e1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list \n",
    "team_name = []\n",
    "for i in team:\n",
    " team_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "418033b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat empty list\n",
    "matches = [] \n",
    "points = []\n",
    "ratings = []\n",
    "new_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dea27da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all(\"td\",class_='rankings-block__banner--matches'): # first place team number of matches\n",
    "    matches.append(i.text)\n",
    "for  i in soup.find_all(\"td\",class_='ranking-block__banner--points'): # first place eam points\n",
    "        points.append(i.text)\n",
    "for i in soup.find_all(\"td\",class_='ranking-block__banner--rating-u-text-right'): #first place team ratings\n",
    "    ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell u-center-text'):# other teams number of matches and points\n",
    "    new_list.append(i.text)\n",
    "for i in range(0,len(new_list)-1,2):\n",
    "    matches.append(new_list[i]) # other teams matches\n",
    "    points.append(new_list[i+1]) # other teams points\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell u-text-right rating'):# other teams ratings\n",
    "    ratings.append(i.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "60903b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Team_name, Matches, Points, Ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make DataFrame for top 10 icc teams\n",
    "icc=pd.DataFrame({})\n",
    "icc['Team_name']=team_name[:10]\n",
    "icc['Matches']=matches[:10]\n",
    "icc['Points']=points[:10]\n",
    "icc['Ratings']=ratings[:10]\n",
    "icc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b4737832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Team_name, Matches, Points, Ratings]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(icc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77506c57",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "815859d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa2d3cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sending a GET request to the URL\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "39d502be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "346cbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list for players, teams, rating\n",
    "bat_players = []\n",
    "teams = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d0821b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\3562568707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Find all the top ten batting players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Find all the top ten batting players\n",
    "table = soup.find(\"table\",class_=\"table\")\n",
    "rows = table.find_all('tr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5e0c6c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\1787417127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Get the top 10 players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"td\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "#Get the top 10 players\n",
    "for row in rows[1:11]:\n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "\n",
    "#Specifying the columns where the data for each variable can be found \n",
    "player_name = columns[1].text.strip()\n",
    "team = columns[2].text.strip()\n",
    "rating = column[3].text.strip()\n",
    "\n",
    "\n",
    "# Adding players name, team and rating to the list\n",
    "bat_players.append(player_name)\n",
    "teams.append(team)\n",
    "ratings.append(rating)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "32cad8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary from extracted data\n",
    "data ={\"Player\":bat_players,\"Team\":teams,\"Rating\":ratings}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c730d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# creat DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# display result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0e60b",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb406607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# send get request to the url\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "\n",
    "# display response\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8c30e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "#display soup\n",
    "#soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e90d27c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\2687326845.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# find all the ten top bowling players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# creat empty list for players, teams and rating\n",
    "bowl_players = []\n",
    "teams = []\n",
    "rating = []\n",
    "\n",
    "# find all the ten top bowling players\n",
    "table = soup.find(\"table\",class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "45841170",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\3521429446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Get 10 top players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"td\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Specifying the columns where the data for each variable can be found\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "#Get 10 top players\n",
    "for row in rows[1:11]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    \n",
    "# Specifying the columns where the data for each variable can be found \n",
    "player_name = columns[1].text.strip()\n",
    "team = columns[2].text.strip()\n",
    "rating = columns[3].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "500e55b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'player_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\2415556266.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Adding players name, team and ratings to the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbowl_players\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mteams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'player_name' is not defined"
     ]
    }
   ],
   "source": [
    "#Adding players name, team and ratings to the list\n",
    "bowl_players.append(player_name)\n",
    "teams.append(team)\n",
    "ratings.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79013845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary from extracted data\n",
    "data = {\"Player\":bowl_players,\"Team\":teams,\"Rating\":ratings}\n",
    "\n",
    "# create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# display the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86759e62",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape details of all the posts from\n",
    "https://www.patreon.com/coreyms .Scrape the \n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52036ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00c918f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request  web page server  to get source of code page\n",
    "response = requests.get(\"https://www.patreon.com/coreyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "91e8ecac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display response\n",
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b0cb6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "\n",
    "#display content\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "118faba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the post\n",
    "posts = soup.find_all('div', class_='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b8e7a122",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3043415229.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\probook 430 G3\\AppData\\Local\\Temp\\ipykernel_12524\\3043415229.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    for post in posts:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# CREATE AN EMPTY LIST\n",
    "data = []\n",
    "        for post in posts:\n",
    "            heading = post.find('h4').text.strip()\n",
    "            date = post.find('time')['datetime']\n",
    "            content = post.find('div', class_='post__excerpt').text.strip()\n",
    "            youtube_link = post.find('a', class_='post__thumbnail')['href']\n",
    "            likes = post.find('div', class_='post__footer').find('button').text.strip()\n",
    "\n",
    "            data.append({'Heading': heading, 'Date': date, 'Content': content, 'YouTube Link': youtube_link, 'Likes': likes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2c5f2f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame from extracted header\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515d9f0",
   "metadata": {},
   "source": [
    "# 5 Write a python program to scrape house details from mentioned URL. It should include house title, location, \n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, \n",
    "Rajaji Nagar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0f845755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# send get request to the url\n",
    "response = requests.get(\"https://www.nobroker.in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "343be754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send get request to the url\n",
    "response = requests.get(\"https://www.nobroker.in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "054dae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display response \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4f9d3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "\n",
    "#display content\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b700126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the house details\n",
    "houses = soup.find_all('div', class_='nb__2JHKO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2597d1cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3575440135.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\probook 430 G3\\AppData\\Local\\Temp\\ipykernel_12524\\3575440135.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    for house in houses:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# create an empty list \n",
    "data = []\n",
    "        for house in houses:\n",
    "            title = house.find('h2', class_='heading-6').text.strip()\n",
    "            location = house.find('div', class_='nb__35Ol7').text.strip()\n",
    "            area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "            emi = house.find('div', class_='font-semi-bold').text.strip()\n",
    "            price = house.find('div', class_='heading-7').text.strip()\n",
    "\n",
    "            data.append({'Title': title, 'Location': location, 'Area': area, 'EMI': emi, 'Price': price})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "43a38741",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\3300418493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# creating a dictionary from the extracted data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m data = {\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"Location\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"Area\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title' is not defined"
     ]
    }
   ],
   "source": [
    "# creating a dictionary from the extracted data \n",
    "data = {\n",
    "    \"Title\":title,\n",
    "    \"Location\":location,\n",
    "    \"Area\":area,\n",
    "    \"Emi\":emi,\n",
    "    \"Price\":price\n",
    "}\n",
    "\n",
    "# creating a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# display result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b77ba",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape first 10 product details which include product name , price , Image URL from \n",
    "https://www.bewakoof.com/bestseller?sort=popular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d4239d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a1fb8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send get request to the url\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7dbbae45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display  response \n",
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "50404ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9769d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = soup.find_all('div', class_='productCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b9f653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list\n",
    "data = []\n",
    "for product in products[:10]:\n",
    "            name = product.find('h3', class_='name').text.strip()\n",
    "            price = product.find('div', class_='productCard_Price').text.strip()\n",
    "            image_url = product.find('img')['src']\n",
    "\n",
    "            data.append({'Name': name, 'Price': price, 'Image URL': image_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "360b5cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, PriceImage URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# create \n",
    "# create DataFrame from extracted header\n",
    "df = pd.DataFrame(data, columns=[\"Name\",\"Price\"\"Image URL\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3f0c",
   "metadata": {},
   "source": [
    "# 7) Please visit https://www.cnbc.com/world/?region=world and scrap- \n",
    "a) headings \n",
    "b) date \n",
    "c) News link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4ba1de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "248630d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Send a GET request to the URL\n",
    "response = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f9b546fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9ec03f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list for headlines, times, news link\n",
    "headlines = []\n",
    "times = []\n",
    "news_links = []\n",
    "\n",
    "#Finding all the news articles on the web page\n",
    "news_articles = soup.find_all(\"div\", class_=\"LatestNews-container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "43206051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through each news and extract the relevant data\n",
    "for article in news_articles:\n",
    "    #Extracting headlines\n",
    "    headline = article.find(\"a\", class_=\"LatestNews-headline\").text.strip()\n",
    "    headlines.append(headline)\n",
    "\n",
    "    #Extracting time\n",
    "    time = article.find(\"time\").text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    #Extracting the link of the news\n",
    "    news_link = article.find(\"a\", class_=\"LatestNews-headline\")[\"href\"]\n",
    "    news_links.append(news_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2a319062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline            Time  \\\n",
      "0   Baltimore engineers begin clearing bridge wrec...     2 Hours Ago   \n",
      "1   Psychologist: 'High-functioning' uses success ...     7 Hours Ago   \n",
      "2   Fitness influencers swear by the 'carnivore di...     7 Hours Ago   \n",
      "3   AT&T is investigating a leak that put millions...     7 Hours Ago   \n",
      "4   Bitcoin could soar to $150,000 this year, hedg...     8 Hours Ago   \n",
      "5   America's 'deskless' hourly wage workers will ...     8 Hours Ago   \n",
      "6   How 26-year-old balances public office, workin...     9 Hours Ago   \n",
      "7   Here are the 3 key things we're most focused o...     9 Hours Ago   \n",
      "8   Pet insurance didn't feel worth it—until my ca...     9 Hours Ago   \n",
      "9   How this mission-driven chocolate company make...     9 Hours Ago   \n",
      "10  Nvidia CEO: Smart people struggle with these 2...     9 Hours Ago   \n",
      "11  Generative AI 'FOMO' is driving tech heavyweig...    10 Hours Ago   \n",
      "12  Decluttering tips from professional organizers...    10 Hours Ago   \n",
      "13  As new vehicles become more like computers, wh...    10 Hours Ago   \n",
      "14  How Maersk grew its shipping empire and how it...    10 Hours Ago   \n",
      "15  Buy these 5 stocks with upside potential as Ap...    11 Hours Ago   \n",
      "16  Crypto investors are bracing for a busy second...    11 Hours Ago   \n",
      "17  Watch out for these cyclical stocks, Morgan St...    11 Hours Ago   \n",
      "18  'Too few college-educated men': A look at why ...    11 Hours Ago   \n",
      "19  Monthly Meeting Q&A with Cramer: From energy t...  March 29, 2024   \n",
      "20  Trump's campaign trail promos for sneakers, bi...  March 29, 2024   \n",
      "21  New report: The 10 richest retirement towns in...  March 29, 2024   \n",
      "22  Just 1% of construction CEOs are women—how thi...  March 29, 2024   \n",
      "23  Microsoft, Salesforce woo skeptical CFOs on ne...  March 29, 2024   \n",
      "24  These were our 4 best — and 4 worst — portfoli...  March 29, 2024   \n",
      "25  How a 36-year-old with 'little experience' bui...  March 29, 2024   \n",
      "26  10 in-demand remote jobs paying over $100,000 ...  March 29, 2024   \n",
      "27  How to answer the updated crypto question when...  March 29, 2024   \n",
      "28  2 simple ways to get people to listen when you...  March 29, 2024   \n",
      "29  Key Fed inflation gauge rose 2.8% annually in ...  March 29, 2024   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2024/03/30/baltimore-engi...  \n",
      "1   https://www.cnbc.com/2024/03/30/why-youre-seei...  \n",
      "2   https://www.cnbc.com/2024/03/30/fitness-influe...  \n",
      "3   https://www.cnbc.com/2024/03/30/att-investigat...  \n",
      "4   https://www.cnbc.com/2024/03/30/hedge-fund-man...  \n",
      "5   https://www.cnbc.com/2024/03/30/why-hourly-wag...  \n",
      "6   https://www.cnbc.com/2024/03/30/meet-bushra-am...  \n",
      "7   https://www.cnbc.com/2024/03/30/the-3-things-w...  \n",
      "8   https://www.cnbc.com/2024/03/30/is-pet-insuran...  \n",
      "9   https://www.cnbc.com/2024/03/30/how-this-missi...  \n",
      "10  https://www.cnbc.com/2024/03/30/nvidia-ceo-the...  \n",
      "11  https://www.cnbc.com/2024/03/30/fomo-drives-te...  \n",
      "12  https://www.cnbc.com/2024/03/30/decluttering-t...  \n",
      "13  https://www.cnbc.com/2024/03/30/as-new-vehicle...  \n",
      "14  https://www.cnbc.com/2024/03/30/how-maersk-gre...  \n",
      "15  https://www.cnbc.com/2024/03/30/goldman-sachs-...  \n",
      "16  https://www.cnbc.com/2024/03/30/crypto-investo...  \n",
      "17  https://www.cnbc.com/2024/03/30/these-stocks-c...  \n",
      "18  https://www.cnbc.com/2024/03/30/a-look-at-why-...  \n",
      "19  https://www.cnbc.com/2024/03/29/monthly-meetin...  \n",
      "20  https://www.cnbc.com/2024/03/29/trump-sellig-b...  \n",
      "21  https://www.cnbc.com/2024/03/29/richest-retire...  \n",
      "22  https://www.cnbc.com/2024/03/29/women-make-up-...  \n",
      "23  https://www.cnbc.com/2024/03/29/microsoft-sale...  \n",
      "24  https://www.cnbc.com/2024/03/29/these-were-our...  \n",
      "25  https://www.cnbc.com/2024/03/29/ltk-founder-am...  \n",
      "26  https://www.cnbc.com/2024/03/29/the-most-in-de...  \n",
      "27  https://www.cnbc.com/2024/03/29/how-to-answer-...  \n",
      "28  https://www.cnbc.com/2024/03/29/simple-ways-to...  \n",
      "29  https://www.cnbc.com/2024/03/29/pce-inflation-...  \n"
     ]
    }
   ],
   "source": [
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Headline\": headlines,\n",
    "    \"Time\": times,\n",
    "    \"News Link\": news_links\n",
    "}\n",
    "#Create dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Display the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8caa58e",
   "metadata": {},
   "source": [
    "# 8)       Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-     \n",
    "articles/ and scrap- \n",
    "a) Paper title \n",
    "b) date \n",
    "c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5254844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "72b64b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Send a GET request to the URL\n",
    "response = requests.get(\"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3c9da24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "670d4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all news articles on the web page\n",
    "article_items = soup.find_all('div', class_='journal_detail_article')\n",
    "\n",
    "# article_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "89a37a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists to store extracted data\n",
    "paper_titles = []\n",
    "authors_list = []\n",
    "dates = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b18a5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating through the article content to extract article details\n",
    "for item in article_items:\n",
    "    # Extracting title of the article\n",
    "    title = item.find('h3').get_text(strip=True)\n",
    "    paper_titles.append(title)\n",
    "    \n",
    "    # Extracting the authors\n",
    "    authors = item.find('p', class_='article-authors').get_text(strip=True)\n",
    "    authors_list.append(authors)\n",
    "    \n",
    "    # Extracting the p date\n",
    "    date = item.find(\"span\", class_='text-muted').get_text(strip=True)\n",
    "    dates.append(date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4da94dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Paper Title\": paper_titles,\n",
    "    \"Authors\": authors_list,\n",
    "    \"Date\": dates,\n",
    "}\n",
    "# Creating a DataFrame from the dictionar\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
